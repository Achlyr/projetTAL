{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import tqdm as notebook_tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "from transformers import pipeline, TFDistilBertForSequenceClassification, DistilBertTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'un document sans annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récupération des données annotées\n",
    "tabA=pd.read_csv(\"edos_labelled_aggregated.csv\",sep=\",\")\n",
    "\n",
    "# Création d'un fichier avec les entrées non annotés\n",
    "with open(\"non_labelled_aggregated.csv\", \"w\", encoding=\"utf-8\") as f_out_total:\n",
    "    f_out_total.write(\"rewire_id,text\\n\")\n",
    "    for i in range(len(tabA)):\n",
    "        id=tabA[\"rewire_id\"].iloc[i]\n",
    "        text = tabA[\"text\"].iloc[i]\n",
    "        text_escaped = text.replace('\"', '\"\"')\n",
    "        f_out_total.write(id+\",\\\"\"+text_escaped+\"\\\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Regex pour les insultes sexistes spécifiques\n",
    "patternInsultsSpe= r'\\b(?!(?:wo)?man\\b|men\\b|women\\b|witch[-\\s]hunts?\\b)(wh?[oaiy]mm?[eayi]*n?[sz]?|puss[yi]?e?s?|cunts?|\\wiy?a?tche?y?s?(ing)?(tard)?|whores?(dom)?(llywood)?|slut(tiness)?s?|hoe?s?|slags?|(trad)?e?thots?(ism)?(dom)?(life)?|femoids?|harp[yi]e?s?|twats?|spinsters?|tranny|roa?sties?|stac[yi]e?s?|[vh]ags?|(noodle)?foids?|feminazis?|d[yi]kes?|skanks?|gold digg(er)?(ing)?s?|bi--th|slutt(ng)?y?)\\b'\n",
    "regexInsultsSpe = re.compile(patternInsultsSpe, re.IGNORECASE)\n",
    "\n",
    "# Regex pour les termes descriptifs\n",
    "patternDsc = r'\\b(?!(?:man|men)\\b)(she|her|\\w*wom[ae]n|wi[v|f]es?|girls?|lad[y|i]e?s?|daughters?|sisters?|girlfriends?|aunti?e?s?|grandmothers?|mothers?|m[o|u]mm?[i|y]?e?s?|females?|feminine|metoo|femininity|feminis[m|t]s?|gals?|chicks?|vaginas?)\\b'\n",
    "regexDsc = re.compile(patternDsc, re.IGNORECASE)\n",
    "\n",
    "# Charger le tokenizer et le modèle qui vont servir à prédire la polarité des entrées\n",
    "model_name = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtre et classification selon la polarité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons décidé d'effectuer notre évaluation sur une quantité limitée d'entrées pour diminuer la durée de traitement. Nous avons notamment pris cette décision car notre première tentative d'évaluation sur la totalité du corpus a pris plus d'une heure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"non_labelled_aggregated.csv\", \"r\", encoding=\"utf-8\") as f_in, open(\"données_annotees_results1000.csv\", \"w\", encoding=\"utf-8\") as f_out:\n",
    "    f_out.write(\"rewire_id,text,label_sexist\\n\")\n",
    "    for index, line in enumerate(f_in, start=1):\n",
    "        if index>=2 and index<=1001:\n",
    "            line=line.strip()\n",
    "            matches = regexInsultsSpe.findall(line)\n",
    "            if len(matches) > 0:\n",
    "                f_out.write(line+\",sexist\\n\")\n",
    "            else:\n",
    "                matches = re.findall(patternDsc, line)\n",
    "                if len(matches) > 0:\n",
    "                    inputs = tokenizer(line, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "                    # Effectuer des prédictions\n",
    "                    outputs = model(inputs.data)\n",
    "                    # Obtenir les scores et les étiquettes prédites\n",
    "                    predictions = tf.nn.softmax(outputs.logits, axis=-1)\n",
    "                    predicted_labels = tf.argmax(predictions, axis=1)\n",
    "                    # Définir le mapping des labels\n",
    "                    label_map = {0: 'positive', 1: 'neutral', 2: 'negative'}\n",
    "                    # Filtrer selon la polarité\n",
    "                    for text, label in zip(line, predicted_labels):\n",
    "                        label_str = label_map[label.numpy()]\n",
    "                        if label_str in ['negative']:\n",
    "                            f_out.write(line+\",sexist\\n\")\n",
    "                        if label_str in ['neutral','positive']:\n",
    "                            f_out.write(line+\",not sexist\\n\")\n",
    "                else:\n",
    "                    f_out.write(line+\",not sexist\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérification des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nous avons bien les 1000 entrées annotées\n",
      "La précision est de 0.3926553672316384\n",
      "Le rappel est de 0.8825396825396825\n",
      "Le fscore est de 0.5434995112414467\n"
     ]
    }
   ],
   "source": [
    "tabB=pd.read_csv(\"données_annotees_results1000.csv\",sep=\",\")\n",
    "#On ne garde que les 1000 premières entrées pour les comparer avec les annotations\n",
    "tabA_1000=tabA.iloc[0:1000]\n",
    "\n",
    "#On vérifie que toutes les entrées ont été annotées\n",
    "print(\"Nous avons bien les\",len(tabB),\"entrées annotées\")\n",
    "\n",
    "# Matrice de confusion\n",
    "PRFSliste=precision_recall_fscore_support(tabA_1000[\"label_sexist\"],tabB[\"label_sexist\"],labels=[\"sexist\",\"not sexist\"])\n",
    "pd.crosstab(tabA_1000[\"label_sexist\"],tabB[\"label_sexist\"],rownames=[\"Valeurs réelles\"],colnames=[\"Valeurs prédites\"])\n",
    "precision=PRFSliste[0][0]\n",
    "rappel=PRFSliste[1][0]\n",
    "fscore=PRFSliste[2][0]\n",
    "print(\"La précision est de \"+str(precision))\n",
    "print(\"Le rappel est de \"+str(rappel))\n",
    "print(\"Le fscore est de \"+str(fscore))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, comme nous nous en doutions d'après nos premiers résultats, nous parvenons à obtenir un assez bon rappel puisque nous avons développé notre algorithme à partir des données annotées, mais l'utilisation de la polarité n'est pas un critère assez précis pour parvenir à une bonne précision du système."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
